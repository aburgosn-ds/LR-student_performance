{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "## Multiple Linear Regression Algorithm Implementation\n",
    "\n",
    "Multiple linear regression finds linear dependence between each feature and the target. A function fits to the data while training.\n",
    "\n",
    "1. Function to adjust:\n",
    "\n",
    "$$ f_{\\vec{w},b}(\\vec{x}^{(i)}) = \\vec{w}\\cdot \\vec{x}^{(i)} + b = w_1x_1^{(i)} + w_2x_2^{(i)} + \\cdots + w_nx_n^{(i)} + b $$\n",
    "\n",
    "2. Cost function for minimization: Squared Error Cost Function.\n",
    "\n",
    "$$ J(\\vec{w}, b) = \\frac{1}{2m} \\sum _{i=1}^m  \\left( f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)} \\right)^2$$\n",
    "\n",
    "3. Gradients:\n",
    "$$ \\frac{\\partial J(\\vec{w},b)}{\\partial w_j} = \\frac{1}{m}\\sum_{i=1}^m(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)}) x_j^{(i)} $$\n",
    "$$ \\frac{\\partial J(\\vec{w},b)}{\\partial b} = \\frac{1}{m}\\sum_{i=1}^m(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)}) $$\n",
    "\n",
    "4. Gradient descent algorithm:\n",
    "<div style=\"margin-left: 44px;\">\n",
    "Repeat until convergence:\n",
    "</div>\n",
    "\n",
    "$$ w_j=w_j-\\alpha \\frac{\\partial J(\\vec{w},b)}{\\partial w_j} $$\n",
    "$$ b=b-\\alpha \\frac{\\partial J(\\vec{w},b)}{\\partial b} $$\n",
    "\n",
    "*Simultaneously update for $w_j \\hspace{1mm} (j=1,...,n) $ and $b$*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation**\n",
    "\n",
    "\n",
    "| **Regression** |     Description    |  Python  |    \n",
    "| -- | -- | -- | \n",
    "|  $\\mathbf{X}$ | training example matrix                  | `X_train` or `X` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` |\n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$ Training Example | `X[i]`, `y[i]` or `x`, `y`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight                      | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $\\alpha$ | learning rate | `alpha` |\n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$ | `f_wb` | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_model(X, w, b):\n",
    "    '''\n",
    "    Computes the model for a set of training examples\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Shape (m,n) m training examples with n features\n",
    "        w (ndarray): Shape (n,) n features weights\n",
    "        b (scalar): bias parameter\n",
    "    \n",
    "    Returns:\n",
    "        f_wb (ndarray): Shape (m,) predicted outputs\n",
    "    \n",
    "    '''\n",
    "    f_wb = np.dot(X,w) + b\n",
    "\n",
    "    return f_wb\n",
    "\n",
    "\n",
    "def compute_cost(X, y, w, b):\n",
    "    '''\n",
    "    Computes squared error cost function for the given training set\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Shape (m,n) training examples with n features\n",
    "        w (ndarray): Shape (n,) n features weights\n",
    "        y (ndarray): Shape (m,) true outputs\n",
    "        b (scalar): bias parameter\n",
    "    \n",
    "    Returns:\n",
    "        J_wb (scalar): cost function value\n",
    "    '''\n",
    "\n",
    "    # Training examples\n",
    "    m = X.shape[0]\n",
    "\n",
    "    loss = linear_model(X, w, b) - y\n",
    "    J_wb = (1./2.*m) * np.sum(loss**2)\n",
    "\n",
    "    return J_wb\n",
    "\n",
    "def compute_gradients(X, y, w, b):\n",
    "    '''\n",
    "    Computes gradients for each weight and bias\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Shape (m,n) training examples with n features\n",
    "        w (ndarray): Shape (n,) n features weights\n",
    "        y (ndarray): Shape (n,) true outputs\n",
    "        b (scalar): bias parameter\n",
    "    \n",
    "    Returns:\n",
    "        dJ_dw (ndarray): Shape (n,) weight gradients\n",
    "        dJ_db (scalar): bias gradient\n",
    "    '''    \n",
    "    m = X.shape[0]\n",
    "\n",
    "    loss = linear_model(X,w,b) - y\n",
    "    dJ_dw = (1./m) * np.sum(np.reshape(loss, (loss.shape[0],1)) * X, axis=0)\n",
    "    dJ_db = (1./m) * np.sum(loss)\n",
    "\n",
    "    return dJ_dw, dJ_db\n",
    "\n",
    "def gradient_descent(X, y, w_init, b_init, max_iter=1000, alpha=1.e-6, epsilon=1.e-3):\n",
    "    '''\n",
    "    Implements gradient descent algorithm \n",
    "    Args:\n",
    "        X (ndarray): Shape (m,n) training examples with n features\n",
    "        y (ndarray): Shape (m,) true outputs\n",
    "        w_init (ndarray): Shape (n,) n features weights\n",
    "        b_init (ndarray): Shape (n,) true outputs\n",
    "        b (scalar): bias parameter\n",
    "        max_inter (scalar): maximun number of gradient descent steps\n",
    "        alpha (scalar): learning rate\n",
    "        epsilon (scalar): defines convergence. Cost function difference between two consecutive interations \n",
    "    \n",
    "    Returns:\n",
    "        w (ndarray): Shape (n,) optimized features weights\n",
    "        b (scalar): optimized bias  \n",
    "    '''\n",
    "    iter = 0\n",
    "\n",
    "    # Initializating parameters\n",
    "    w = np.copy(w_init)\n",
    "    b = np.copy(b_init)\n",
    "\n",
    "    J_wb = 0.\n",
    "    J_hist = []\n",
    "    \n",
    "    # Repeating gradient descent algorithm until max_iter or convergence \n",
    "    while iter <= max_iter:\n",
    "        iter += 1\n",
    "\n",
    "        # Gradients\n",
    "        dJ_dw, dJ_db = compute_gradients(X, y, w, b)\n",
    "        \n",
    "        # Update parameters\n",
    "        w -= alpha * dJ_dw\n",
    "        b -= alpha * dJ_db\n",
    "\n",
    "        # Cost function with updated parameters\n",
    "        J_wb_curr = compute_cost(X, y, w, b)\n",
    "        J_hist.append(J_wb_curr)\n",
    "\n",
    "        # Check if convergence achieved\n",
    "        consecutive_diff = abs(J_wb_curr - J_wb)\n",
    "        if consecutive_diff <= epsilon:\n",
    "            print(f\"Convergence achieved in {iter} iterations.\")\n",
    "            break\n",
    "        \n",
    "        J_wb = J_wb_curr\n",
    "    else:\n",
    "        print(f\"Convergence not achieved with {max_iter} iteration.\")    \n",
    "    \n",
    "    return w, b, J_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Alexander Burgos\n",
    "\n",
    "Fecha: 2025-02-10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
